[
{
		"title": "[24.02.04] 일상",
		"date":"Thu Apr 04 2024 10:49:07 GMT+0000 (Coordinated Universal Time)",
		"url":"/Blogging/daily/24-02-04/",
		"content": "간만에 브라우니70 본점에 와서 스터디 하는 중.\n공부 할 것이 많다...\n#daily #일상",
		"tags": ["daily", "일상", "note"]
},

{
		"title": "24-02-08",
		"date":"Thu Apr 04 2024 10:49:07 GMT+0000 (Coordinated Universal Time)",
		"url":"/Blogging/daily/24-02-08/",
		"content": "새해 복 많이 받으세요.",
		"tags": [ "note"]
},

{
		"title": "1장 GPT-4, ChatGPT, 랭체인 개요",
		"date":"Thu Apr 04 2024 10:49:07 GMT+0000 (Coordinated Universal Time)",
		"url":"/Blogging/study/books/GPT-4+ChatGPT+라마인덱스+랭체인을 활용한 인공지능 프로그래밍/1장/",
		"content": "GPT-4와 ChatGPT 및 랭체인\nChatGPT란?\nGPT-4와 GPT-3.5란?\n대규모 언어 모델이란?\nOpenAI API란?\n라마인덱스란?\n랭체인이란?\n대규모 언어 모델의 활용 사례\n인공지능과 머신러닝 및 딥러닝\n#chatgpt #langchain #랭체인 #라마인덱스 #인공지능 #머신러닝 #딥러닝",
		"tags": ["chatgpt", "langchain", "랭체인", "라마인덱스", "인공지능", "머신러닝", "딥러닝", "note"]
},

{
		"title": "6장 랭체인(LangChain)",
		"date":"Thu Apr 04 2024 10:49:07 GMT+0000 (Coordinated Universal Time)",
		"url":"/Blogging/study/books/GPT-4+ChatGPT+라마인덱스+랭체인을 활용한 인공지능 프로그래밍/2장/",
		"content": "랭체인이란?\n\n언어 모델을 기반으로 하는 애플리케이션을 개발하기 위한 프레임워크\nLangChain 라이브러리\n\nPython 및 JavaScript 라이브러리. 수많은 구성 요소에 대한 인터페이스 및 통합, 이러한 구성 요소를 체인 및 에이전트로 결합하기 위한 기본 런타임, 체인 및 에이전트의 구성\n\n서비스 구조\n\nLangChain 템플릿\n\n다양한 작업을 위해 쉽게 배포할 수 있는 참조 아키텍처 모음\n\nLangServe\n\nLangChain 체인을 REST API로 배포하기 위한 라이브러리\n\nLangSmith\n\nLLM 프레임워크에 구축된 체인을 디버그, 테스트, 평가 및 모니터링하고 LangChain과 원활하게 통합할 수 있는 개발자 플랫폼\n\n개발\n\nLangChain/LangChain.js에 애플리케이션을 작성\n\n생산화\n\nLangSmith를 사용하여 체인을 검사, 테스트 및 모니터링\n\n배포\n\nLangServe를 사용하여 모든 체인을 API로 전환\n\n모듈\n\nLLM\n\nLLM 호출을 위한 공통 인터페이스\n\n프롬프트 템플릿\n\n사용자 입력에 따른 프롬프트 생성\n\n체인\n\n여러 LLM과 프롬프트의 입출력을 연결\n\n에이전트\n\n사용자의 요청에 따라 어떤 기능을 어떤 순서로 실행할 것인지 결정\n\n도구\n\n에이전트가 수행하는 특정 기능\n\n메모리\n\n체인 및 에이전트의 메모리 보유\n\n설치\npip install langchain\n\nLLM 예제\n\nLLM 호출을 위한 공통 인터페이스\n\n!pip install langchain\n!pip install openai\n\nimport os\nos.environ[&quot;OPENAI_API_KEY&quot;] = &quot;APIKEY&quot;\n\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0.9)\n\nprint(llm(&quot;컴퓨터 게임을 만드는 새로운 한국어 회사명을 하나 제안해 주세요.&quot;); //비바게임즈\n\n프롬프트 템플릿 예제\n\n프롬프트 템플릿은 사용자 입력으로 프롬프트를 생성하기 위한 템플릿\n\nfrom langchain.prompts import PromptTemplate\nfrom langchain.chains import LLMChain\n\nprompt = PromptTemplate(\n\tinput_variables=[&quot;product&quot;],\n\ttemplate=&quot;{product}을 만드는 새로운 한국어 회사명을 하나 제안해 주세요.&quot;,\n)\n\nprint(prompt.format(product=&quot;가정용 로봇&quot;))\n\n체인 사용법\n\n여러 개의 LLM이나 프롬프트의 입출력을 연결하기 위한 모듈\n\nfrom langchain.chains import LLMChain\nfrom langchain.llm import OepnAI\nfrom langchin.prompts import PromptTemplate\n\nprompt = PromptTemplate(\n\tinput_variables=[&quot;product&quot;],\n\ttemplate=&quot;{product}을 만드는 새로운 한국어 회사명을 하나 제안해 주세요.&quot;,\n)\n\nchain = LLMChain(\n\tllm=OpenAI(temperature=0.9),\n\tprompt=prompt\n)\n\n# 싱글 input, output일때는 run을 쓸수 있다.\nchain.run(&quot;가정용 로봇&quot;)\n\n# 여러 input, output일때는 call을 쓴다.\nchain.call({product:&quot;가정용 로봇&quot;})\n\n에이전트와 도구 사용법\n\n사용자의 요청에 따라 어떤 기능을 어떤 순서로 실행할지 결정하는 모듈\n체인은 미리 정해져 있는 기능을 실행하는 반면, 에이전트는 사용자의 요청에 따라 수해오디는 기능\n예시로 웹 검색과 수치 계산을 진행합니다.\n\nSerpAPI, llm-math\n\n!pip install google-search-results\n\nimport os\nos.environ[&quot;SERPAPI_API_KEY&quot;] = &quot;SERPAPI_KEY&quot;\n\nfrom langchain.agents import load_tools\nfrom langchain.llms import OpenAI\n\ntools = load_tools(\n\ttools_names=[&quot;serpapi&quot;,&quot;llm-math&quot;],\n\tllm-OpenAI(temperature=0)\n)\n\nfrom langchain.agents import initialize_agent\n\nagent = initialize_agent(\n\tagent=&quot;zero-shot-react-description&quot;,\n\tllm=OpenAI(temperature=0),\n\ttools=tools,\n\tverbose=True\n)\n\n# llm이 질문을 판단하여 llm-math를 사용합니다.\nagnet.run(&quot;123*4를 계산기로 계산하세요.&quot;)\n\n# llm이 질문을 판단하여 SerpAPI를 사용합니다.\nagent.run(&quot;오늘 한국 서울 날시를 웹 검색으로 확인하세요.&quot;);\n\n메모리 사용 예제\n\n체인이나 에이턴트의 과거 기억을 보관하는 모듈\n\n이전 대화를 기억하고 그 정보를 현재 대화에 반영\n\nfrom langchain.chains import ConversationChain\nfrom langchain.llms import OpenAI\n\nchain = ConversationChain(\n\tllm=OpenAI(temperature=0),\n\tverbose=True\n)\n\nUse cases\nhttps://python.langchain.com/docs/use_cases\nLangServe\n\n개발자가 LangChain 실행 가능 파일과 체인을 REST API로 배포하는 데 도움\nLangChain 애플리케이션의 원클릭 배포를 위해 LangServe의 호스팅 버전을 출시할 예정\n\n특징\n\n사용자의 LangChain 개체에서 자동으로 추론된 입력 및 출력 스키마는 API 호출마다 강제되며, 풍부한 오류 메시지와 함께 사용됩니다.\nJSONSchema 및 Swagger가 포함된 API 문서 페이지 (예시 링크 삽입)\n단일 서버에서 많은 동시 요청을 지원하는 효율적인 /invoke/, /batch/ 및 /stream/ 엔드포인트\n체인/에이전트에서 모든 (또는 일부) 중간 단계를 스트리밍하는 /stream_log/ 엔드포인트\n0.0.40 버전부터는 astream_events를 지원하여 stream_log의 출력을 구문 분석할 필요 없이 쉽게 스트리밍할 수 있습니다.\n스트리밍 출력과 중간 단계를 제공하는 Playground 페이지 (/playground/)\nLangSmith에 내장된 (옵션) 추적 기능, API 키를 추가하기만 하면 됩니다.\nFastAPI, Pydantic, uvloop 및 asyncio와 같은 검증된 오픈 소스 Python 라이브러리를 사용하여 모두 구축되었습니다.\n클라이언트 SDK를 사용하여 LangServe 서버를 로컬에서 실행 중인 Runnable처럼 호출하거나 HTTP API를 직접 호출할 수 있습니다.\nLangServe Hub\n\n설치\npip install &quot;langserve[all]&quot;\n\n예제\n\nOpenAI 및 Anthropic 채팅 모델을 예약하는 LLM 최소 예입니다. 비동기를 사용하고 일괄 처리 및 스트리밍을 지원합니다.\n\n# Server\n&quot;&quot;&quot;Example LangChain server exposes multiple runnables (LLMs in this case).&quot;&quot;&quot;\n\nfrom fastapi import FastAPI\nfrom langchain.chat_models import ChatAnthropic, ChatOpenAI\n\nfrom langserve import add_routes\n\napp = FastAPI(\ntitle=&quot;LangChain Server&quot;,\nversion=&quot;1.0&quot;,\ndescription=&quot;Spin up a simple api server using Langchain's Runnable interfaces&quot;,\n)\n\nadd_routes(\napp,\nChatOpenAI(),\npath=&quot;/openai&quot;,\n)\nadd_routes(\napp,\nChatAnthropic(),\npath=&quot;/anthropic&quot;,\n)\n\nif __name__ == &quot;__main__&quot;:\nimport uvicorn\n\nuvicorn.run(app, host=&quot;localhost&quot;, port=8000)\n\n# Client\nfrom langchain.prompts.chat import ChatPromptTemplate\nfrom langserve import RemoteRunnable\n\nopenai_llm = RemoteRunnable(&quot;&lt;http://localhost:8000/openai/&gt;&quot;)\nanthropic = RemoteRunnable(&quot;&lt;http://localhost:8000/anthropic/&gt;&quot;)\n\n# We can use either LLM\nprompt = ChatPromptTemplate.from_messages(\n[\n(\n&quot;system&quot;,\n&quot;You are a highly educated person who loves to use big words. &quot;\n+ &quot;You are also concise. Never answer in more than three sentences.&quot;,\n),\n(&quot;human&quot;, &quot;Tell me about your favorite novel&quot;),\n]\n).format_messages()\n\n# AIMessage(content=&quot; My favorite novel is Moby Dick by Herman Melville. The intricate plot and rich symbolism make it a complex and rewarding read. Melville's masterful prose vividly evokes the perilous life of whalers on 19th century ships.&quot;, additional_kwargs={}, example=False)\nanthropic.invoke(prompt)\n\n# My favorite novel is Moby-Dick by Herman Melville. The epic tale of Captain Ahab's quest to find and destroy the great white whale is a masterwork of American literature. Melville's dense, philosophical prose and digressive storytelling style make the novel a uniquely challenging and rewarding read.\nfor chunk in anthropic.stream(prompt):\nprint(chunk.content, end=&quot;&quot;, flush=True)\n\n# My favorite novel is The Art of Language by Maximo Quilana. It is a philosophical treatise on the beauty and complexity of human speech. The prose is elegant yet precise.\nasync for chunk in anthropic.astream(prompt):\nprint(chunk.content, end=&quot;&quot;, flush=True)\n\n# As with regular runnables, async invoke, batch and async batch variants are available by default\nopenai_llm.invoke(prompt)\n# AIMessage(content='My favorite novel is &quot;Ulysses&quot; by James Joyce. It\\\\'s a complex and innovative work that explores the intricacies of human consciousness and the challenges of modernity in a highly poetic and experimental manner. The prose is richly layered and rewards careful reading.', additional_kwargs={}, example=False)\nawait openai_llm.ainvoke(prompt)\n\n#[AIMessage(content=&quot; My favorite novel is Moby Dick by Herman Melville. The epic tale of Captain Ahab's obsessive quest to kill the great white whale is a profound meditation on man's struggle against nature. Melville's poetic language immerses the reader in the mysticism of the high seas.&quot;, additional_kwargs={}, example=False),\n# AIMessage(content=&quot; My favorite novel is Moby Dick by Herman Melville. The intricate details of whaling, though tedious at times, serve to heighten the symbolism and tension leading to the epic battle between Captain Ahab and the elusive white whale. Melville's sublime yet economical prose immerses the reader in a turbulent seascape teeming with meaning.&quot;, additional_kwargs={}, example=False)]\nanthropic.batch([prompt, prompt])\n\n# Streaming is available by default\n\n# [AIMessage(content=' Here is a concise description of my favorite novel in three sentences:\\\\n\\\\nMy favorite novel is Moby Dick by Herman Melville. It is the epic saga of the obsessed Captain Ahab pursuing the white whale that crippled him through the seas. The novel explores deep philosophical questions through rich symbols and metaphors.', additional_kwargs={}, example=False),\n# AIMessage(content=&quot; My favorite novel is Moby Dick by Herman Melville. The epic tale of Captain Ahab's obsessive quest for the great white whale is a masterpiece of American literature. Melville's writing beautifully evokes the mystery and danger of the high seas.&quot;, additional_kwargs={}, example=False)]\nawait anthropic.abatch([prompt, prompt])\n\nfrom langchain.schema.runnable import RunnablePassthrough\ncomedian_chain = (\nChatPromptTemplate.from_messages(\n[\n(\n&quot;system&quot;,\n&quot;You are a comedian that sometimes tells funny jokes and other times you just state facts that are not funny. Please either tell a joke or state fact now but only output one.&quot;,\n),\n]\n)\n| openai_llm\n)\n\njoke_classifier_chain = (\nChatPromptTemplate.from_messages(\n[\n(\n&quot;system&quot;,\n&quot;Please determine if the joke is funny. Say `funny` if it's funny and `not funny` if not funny. Then repeat the first five words of the joke for reference...&quot;,\n),\n(&quot;human&quot;, &quot;{joke}&quot;),\n]\n)\n| anthropic\n)\n\nchain = {&quot;joke&quot;: comedian_chain} | RunnablePassthrough.assign(\nclassification=joke_classifier_chain\n)\n\n# {'joke': AIMessage(content=&quot;Why don't scientists trust atoms?\\\\n\\\\nBecause they make up everything!&quot;, additional_kwargs={}, example=False),\n# 'classification': AIMessage(content=&quot; not funny\\\\nWhy don't scientists trust atoms?&quot;, additional_kwargs={}, example=False)}\nchain.invoke({})\n\nLangSmith\n\n프로덕션급 LLM 애플리케이션을 구축하기 위한 플랫폼\n새로운 체인, 에이전트 또는 도구 세트를 신속하게 디버깅\n미세 조정, 몇 번의 메시지 표시 및 평가를 위한 데이터 세트 생성 및 관리\n자신 있게 개발하려면 애플리케이션에서 회귀 테스트를 실행하세요.\n제품 통찰력과 지속적인 개선을 위한 생산 분석 캡처\n\nLangSmith와 Lilac으로 LLM을 fine-tuning\nhttps://blog.langchain.dev/fine-tune-your-llms-with-langsmith-and-lilac/\nOpenAI Fine-Tuning\nfrom langsmith import Client\n\nclient = Client()\n\nimport datetime\n\nproject_name = &quot;default&quot;\nrun_type = &quot;llm&quot;\nend_time = datetime.datetime.now()\n\nruns = client.list_runs(\nproject_name=project_name,\nrun_type=run_type,\nerror=False,\n)\n\nfrom langchain import chains, chat_models, prompts, schema, callbacks\n\nchain = prompts.ChatPromptTemplate.from_template(&quot;Tell a joke for:\\\\n{input}&quot;) | chat_models.ChatAnthropic(tags=['my-anthropic-run']) | schema.output_parser.StrOutputParser()\n\nwith callbacks.collect_runs() as cb:\nchain.invoke({&quot;input&quot;: &quot;foo&quot;})\n# Assume feedback is logged\nrun = cb.traced_runs[0]\nclient.create_feedback(run.id, key=&quot;user_click&quot;, score=1)\n\nproject_name = &quot;default&quot;\nend_time = datetime.datetime.now()\n\nruns = client.list_runs(\nproject_name=project_name,\nexecution_order=1,\nfilter='and(eq(feedback_key, &quot;user_click&quot;), eq(feedback_score, 1))',\n# For continuous scores, you can filter for &gt;, &lt;, &gt;=, &lt;= with the followingg arguments: gt/lt/gte/lte(feedback_score, 0.9)\n# filter='and(eq(feedback_key, &quot;user_click&quot;), gt(feedback_score, 0.9))',\nerror=False,\n)\n\nllm_runs = []\nfor run in runs:\nllm_run = next(client.list_runs(project_name=project_name, run_type=&quot;llm&quot;, parent_run_id=run.id))\nllm_runs.append(llm_run)\n\nllm_runs[0].tags\n\n# For any &quot;Chain&quot; object, you can add tags directly on the Example with LLMChain\nimport uuid\n\nunique_tag = f&quot;call:{uuid.uuid4()}&quot;\n\nchain = chains.LLMChain(\nllm=chat_models.ChatAnthropic(tags=['my-cool-llm-tag']), # This tag will only be applied to the LLM\nprompt=prompts.ChatPromptTemplate.from_template(&quot;Tell a joke based on the following prompt:\\\\n\\\\nPrompt:{input}&quot;),\ntags=[&quot;my-tag&quot;]\n)\n\n# You can also define at call time for the call/invoke/batch methods.\n# This tag will be propagated to all child calls\nprint(chain({&quot;input&quot;: &quot;podcasting these days&quot;}, tags=[unique_tag]))\n\n# If you're defining using Runnables (aka langchain expression language)\nrunnable = (\nprompts.ChatPromptTemplate.from_template(&quot;Tell a joke based on the following prompt:\\\\n\\\\nPrompt:{input}&quot;)\n| chat_models.ChatAnthropic(tags=['my-cool-llm-tag']) # This tag will only be applied to the LLM\n| schema.StrOutputParser(tags=['some-parser-tag'])\n)\n\n# Again, you can tag at call time as well. This tag will be propagated to all child calls\nprint(runnable.invoke({&quot;input&quot;: &quot;podcasting these days&quot;}, {&quot;tags&quot;: [unique_tag]}))\n\nproject_name = &quot;default&quot;\nend_time = datetime.datetime.now()\n\nruns = client.list_runs(\nexecution_order=1, # Only return the root trace\nfilter=f'has(tags, &quot;{unique_tag}&quot;)',\n)\nlen(list(runs))\n\nproject_name = &quot;default&quot;\nrun_type = &quot;llm&quot;\nend_time = datetime.datetime.now()\n\nruns = client.list_runs(\nproject_name=project_name,\nrun_type=run_type,\nfilter='eq(name, &quot;ChatAnthropic&quot;)',\nerror=False,\n)\n\n# Example chain for the following query\nfrom langchain import prompts, chat_models\n\nchain = (\nprompts.ChatPromptTemplate.from_template(\n&quot;Summarize the following chat log: {input}&quot;\n)\n| chat_models.ChatOpenAI()\n)\n\nchain.invoke({&quot;input&quot;: &quot;hi there, hello....&quot;})\n\nimport datetime\n\nproject_name = &quot;default&quot;\nrun_type = &quot;prompt&quot;\nend_time = datetime.datetime.now()\n\nruns = client.list_runs(\nproject_name=project_name,\nrun_type=run_type,\nend_time=end_time,\nerror=False,\n)\n\n# You can then get a sibling LLM run by searching by parent_run_id and including other criteria\nfor prompt_run in runs:\nllm_run = next(client.list_runs(project_name=project_name, run_type=&quot;llm&quot;, parent_run_id=prompt_run.parent_run_id))\ninputs, outputs = prompt_run.inputs, llm_run.outputs\n\ndataset = client.create_dataset(\ndataset_name = &quot;Fine-Tuning Dataset Example&quot;,\ndescription=f&quot;Chat logs taken from project {project_name} for fine-tuning&quot;,\ndata_type=&quot;chat&quot;,\n)\nfor run in runs:\nif 'messages' not in run.inputs or not run.outputs:\n# Filter out non chat runs\ncontinue\ntry:\n# Convenience method for creating a chat example\nclient.create_example_from_run(\ndataset_id=dataset.id,\nrun=run,\n)\n# Or if you want to select certain keys/values in inputs\n# inputs = convert_inputs(run.inputs)\n# outputs = convert_outputs(run.outputs)\n# client.create_example(\n# dataset_id=dataset.id,\n# inputs=inputs,\n# outputs=outputs,\n# run=run,\n# )\nexcept:\n# Duplicate inputs raise an exception\npass\n\nfrom langsmith import schemas\nfrom langchain import load\n\ndef convert_messages(example: schemas.Example) -&gt; dict:\nmessages = load.load(example.inputs)['messages']\nmessage_chunk = load.load(example.outputs)['generations'][0]['message']\nreturn {&quot;messages&quot;: messages + [message_chunk]}\n\nmessages = [\nconvert_messages(example)\nfor example in client.list_examples(dataset_name=&quot;Fine-Tuning Dataset Example&quot;)\n]\n\nfrom langchain.adapters import openai as openai_adapter\n\nfinetuning_messages = openai_adapter.convert_messages_for_finetuning(messages)\n\nimport time\nimport json\nimport io\n\nimport openai\n\nmy_file = io.BytesIO()\nfor group in finetuning_messages:\nif any([&quot;function_call&quot; in message for message in group]):\ncontinue\nmy_file.write((json.dumps({&quot;messages&quot;: group}) + &quot;\\\\n&quot;).encode('utf-8'))\n\nmy_file.seek(0)\ntraining_file = openai.File.create(\nfile=my_file,\npurpose='fine-tune'\n)\n\n# Wait while the file is processed\nstatus = openai.File.retrieve(training_file.id).status\nstart_time = time.time()\nwhile status != &quot;processed&quot;:\nprint(f&quot;Status=[{status}]... {time.time() - start_time:.2f}s&quot;, end=&quot;\\\\r&quot;, flush=True)\ntime.sleep(5)\nstatus = openai.File.retrieve(training_file.id).status\nprint(f&quot;File {training_file.id} ready after {time.time() - start_time:.2f} seconds.&quot;)\n\njob = openai.FineTuningJob.create(\ntraining_file=training_file.id,\nmodel=&quot;gpt-3.5-turbo&quot;,\n)\n\n# It may take 10-20+ minutes to complete training.\nstatus = openai.FineTuningJob.retrieve(job.id).status\nstart_time = time.time()\nwhile status != &quot;succeeded&quot;:\nprint(f&quot;Status=[{status}]... {time.time() - start_time:.2f}s&quot;, end=&quot;\\\\r&quot;, flush=True)\ntime.sleep(5)\njob = openai.FineTuningJob.retrieve(job.id)\nstatus = job.status\n\nfrom langchain import chat_models, prompts\n\nmodel_name = job.fine_tuned_model\n# Example: ft:gpt-3.5-turbo-0613:personal::5mty86jblapsed\nmodel = chat_models.ChatOpenAI(model=model_name)\nchain.invoke({&quot;input&quot;: &quot;Who are you designed to assist?&quot;})\n\nLangGraph\n\n구축된(그리고 함께 사용하도록 의도된) LLM을 사용하여 상태 저장 다중 행위자 애플리케이션을 구축하기 위한 라이브러리\n주요 용도는 LLM 어플리케이션에 사이클을 추가하는 것입니다.\n\n#langchain #랭체인 #chatgpt",
		"tags": ["langchain", "랭체인", "chatgpt", "note"]
},

{
		"title": "1장 텍스트 마이닝 기초",
		"date":"Thu Apr 04 2024 10:49:07 GMT+0000 (Coordinated Universal Time)",
		"url":"/Blogging/study/books/파이썬 텍스트 마이닝 완벽 가이드/1장/",
		"content": "1장 텍스트 마이닝 기초\n1.1. 텍스트 마이닝의 정의\n\n무엇인가?\n\n텍스트에서 고품질 정보를 추출하는 과정\n이렇게 추출한 대규모 데이터에서 패턴을 찾는 것.\n\n패턴이란?\n\n예시) 어떤 물건을 선택한 고객이 다른 물건을 함께 구매할 확률을 알아내서 그에 따른 쿠폰이나 상품을 보여주는 것\n\n어떻게?\n\n통계적 패턴 학습\n\n머신러닝이 이에 해당\n\n다만 머신러닝은 정형화된 데이터가 필요하기에 비정형화된 데이터를 정형화 하는 작업이 필요\n\n정리\n\n자연어 처리 기법을 이용해 텍스트를 정형화된 데이터로 변환하고, 머신러닝 기법을 적용해 우리가 관심이 있는 어떤 사건을 예측하고자 하는 방법론\n\n정형화된 데이터의 형태는 일정 길이의 벡터\n\n이렇게 일정 길이의 벡터로 변환 하는 것을 **임베딩**이라고 함\n\n1.2. 텍스트 마이닝 트렌드 변화\n\n여기서는 어떤 것이 있는지만 얘기. 자세한 것은 다른 장에서 다룰 예정\n카운트 기반 문서 표현\n\n딥러닝 전 얘기이며, 문장에 있는 단어들의 개수를 세고, 주로 사용된 단어들을 이용해 그 문장의 내용을 파악하는것\n단어들의 빈도를 이용한 하나의 벡터로 단번에 문서 전체를 표현\n벡터화\n\n단어 : [A, B, C, D, E]\n횟수 : [3, 7, 4, 2, 5]\n\n단점으로는 사용된 단어가 많으면 벡터의 크기가 많이 커진다.\n\n시퀀스 기반의 문서 표현\n\n카운트 기반 문서 표현의 문제점을 해결\n사람이 글을 읽고 이해하는 것과 유사한 방법으로 텍스트의 문맥을 이해하고자 하는 방식\n각 단어를 먼저 벡터화 하고, 벡터의 연속된 나열 혹은 시퀀스로 문서 표현\n원 핫 인코딩 : 단어를 일정 규칙에 따라 정렬하고 단어의 수만큼의 벡터를 만들고 단어 자신의 위치만 1로 표시 하는것.\n예시\n\n단어 : [A, B, C, D, E]\nA : [1, 0, 0, 0, 0]\nB : [0, 1, 0, 0, 0]\nAB: [[1, 0, 0, 0, 0],[0, 1, 0, 0, 0]]\n\n단점\n\n단어를 벡터로 변환 했을때 벡터가 지나치게 커진다.\n문서 또는 문장이 가변의 길이인 단어로 되어 있기에 벡터의 시퀀스 길이가 제각각임\n\n머신러닝이나 딥러닝은 가변 길이의 입력을 보통은 허용하지 않으며, 일정 길이를 기준으로 짧은것은 null이나 0으로 채우고 길면 짤라서 사용한다.\n\n용어 정리\n\n텍스트, 문서, 문장\n\n텍스트, 문서 : 하나의 일관된 목적 혹은 주제를 가지고 쓰여진 글\n문장 : 생각이나 감정을 말로 표현할때 완결된 내용을 나타내는 최소 단위\n\n말뭉치 : 언어 연구를 위해 컴퓨터가 텍스트를 가공, 처리, 분석 할 수 있는 형태로 모아놓은 자료의 집합 or 자연언어 연구를 위해 특정한 목적을 가지고 언어의 표본을 추출한 집단\n\n1.3. 텍스트 마이닝에 필요한 지식과 도구\n\n자연어 처리 기법\n\n컴퓨터를 이용해 사람의 자연어를 분석하고 처리하는 기술\n위키에서는 아래와 같이 처리 기법을 얘기함\n\n형태소 분석\n품사 부착\n구절 단위 분석\n구문 분석\n\n이 책에서는 텍스트 전처리를 위한 기법을 아래와 같이 얘기함\n\n토큰화 (tokenize)\n어간 추출 (stemming)\n표제어 추출(lemmatize)\n정규화 (Normalization)\n품사 태킹 (POS-tagging)\n\n라이브러리\n\nNLTK (Natural Langauge Tool-kit)\nKoNLPy\n\n통계학과 선형대수\n\n텍스트로부터 고품질의 정보를 추출하는데 다양한 통계적 분석 방법 쓰임\n회귀분석, SVM과 같은 머신러닝 방법론을 이해하기 위해서는 통계학이 필수\n대용량의 데이터를 다루려면 행렬도 잘 알아야함.\n라이브러리\n\nNumpy\nPandas\n\n시각화 기법\n\n전달하고자 하는 내용을 한눈에 쉽게 이해시키려고 사용\n텍스트 마이닝에서는 막대그래프와 워드클라우드와 같은 기법을 많이 사용\n라이브러리\n\nmatplotlib\nseaborn\n\n머신러닝\n\n공통적인 알고리즘을 데이터에 적용해 주어진 데이터에 적합한 문제해결 방안을 생성하는 방식\n기계학습\n\n지도학습\n\n회귀\n분류\n\n비지도 학습\n\n클러스터링\n차원축소\n\n강화학습\n\n라이브러리\n\n사이킷런\n\n딥러닝\n\n머신러닝의 한 분류에 속하는 인공신경망에서 은닉층을 깊게 쌓은 신경망 구조를 활용해 학습하는 알고리즘\n이미지 인식, 자연어 처리, 음성 처리 등에서 뛰어난 결과를 보이고 있음\n매우 어려움\n초기에는 RNN, LSTM, CNN등 단순하고 쉬운 방법론이 있었지만\n트랜스포머(Transformer)가 등장하면서 BERT, GPT와 같은 복잡한 기술이 등장\n누군가가 미리 학습된 모형을 가져와서 사용하기에 학습 부담이 많이 줄어듬\n\n전체 학습을 하지 않아도 되어서 시간도 절약됨\n기본 학습 모델에 나의 데이터를 이용해 미세조정(Fint-tunning)을 수행하면 자원 소모가 줄어든다.\n\n라이브러리\n\nKeras\nPytorch\nhuggingface에서 제공하는 SDK\n\n이건 책에 없지만 더 간단하게 허깅페이스에 등록된 모델을 쉽게 가져와 쓰기 좋아서 쓸 예정\n\n1.4. 텍스트 마이닝의 주요 적용 분야\n\n문서 분류\n문서 생성\n문서요약\n질의응답\n기계번역\n토픽 모델링\n\n1.5. 이 책의 실습 환경과 사용 소프트웨어\n\nwin 10\npython 3.8\nNLTK\nscikit-learn\nNumpy\nPandas\nAnaconda\nKoNLPy\nTextBlob\nAFINN\nVADER\nGensim\nTensorflow\nPytorch",
		"tags": [ "note"]
},

{
		"title": "asynchronous (비동기)",
		"date":"Thu Apr 04 2024 10:49:07 GMT+0000 (Coordinated Universal Time)",
		"url":"/Blogging/study/cs/asynchronous/",
		"content": "요청과 그 결과가 분리되어 처리되는 것\n요청을 보낸 후 다른 작업을 수행하다가 결과가 돌아오면 처리하는 것\n대규모 데이터 처리나 네트워크 통신 들에서 유용하게 사용",
		"tags": [ "note"]
},

{
		"title": "process (프로세스)",
		"date":"Thu Apr 04 2024 10:49:07 GMT+0000 (Coordinated Universal Time)",
		"url":"/Blogging/study/cs/process/",
		"content": "실행 중인 프로그램의 인스턴스\n자체 메모리 공간(코드, 데이터, 힙, 스택)을 가지며, 운영체제의 관리 하에 독립적으로 실행",
		"tags": [ "note"]
},

{
		"title": "synchronous (동기)",
		"date":"Thu Apr 04 2024 10:49:07 GMT+0000 (Coordinated Universal Time)",
		"url":"/Blogging/study/cs/synchronous/",
		"content": "요청과 그 결과가 한번에 처리되는 것\n요청을 보낸 후 결과가 돌아올 때까지 대기하는 것",
		"tags": [ "note"]
},

{
		"title": "thread (스레드)",
		"date":"Thu Apr 04 2024 10:49:07 GMT+0000 (Coordinated Universal Time)",
		"url":"/Blogging/study/cs/thread/",
		"content": "",
		"tags": [ "note"]
},

{
		"title": "Javascript에 대해서",
		"date":"Thu Apr 04 2024 10:49:07 GMT+0000 (Coordinated Universal Time)",
		"url":"/Blogging/study/language/Javascript에 대해서/",
		"content": "",
		"tags": [ "note"]
},

{
		"title": "Cache",
		"date":"Thu Apr 04 2024 10:49:07 GMT+0000 (Coordinated Universal Time)",
		"url":"/Blogging/study/web/Cache/",
		"content": "",
		"tags": [ "note"]
},

{
		"title": "DNS",
		"date":"Thu Apr 04 2024 10:49:07 GMT+0000 (Coordinated Universal Time)",
		"url":"/Blogging/study/web/DNS/",
		"content": "",
		"tags": [ "note"]
},

{
		"title": "Domain",
		"date":"Thu Apr 04 2024 10:49:07 GMT+0000 (Coordinated Universal Time)",
		"url":"/Blogging/study/web/Domain/",
		"content": "",
		"tags": [ "note"]
},

{
		"title": "HSTS",
		"date":"Thu Apr 04 2024 10:49:07 GMT+0000 (Coordinated Universal Time)",
		"url":"/Blogging/study/web/HSTS/",
		"content": "HSTS는 HTTP Strict Transport Security로 Web Site에 접속할 때, 강제적으로 HTTPS Protocol로만 접속하게 하는 기능입니다.",
		"tags": [ "note"]
},

{
		"title": "HTTP",
		"date":"Thu Apr 04 2024 10:49:07 GMT+0000 (Coordinated Universal Time)",
		"url":"/Blogging/study/web/HTTP/",
		"content": "",
		"tags": [ "note"]
},

{
		"title": "HTTPS",
		"date":"Thu Apr 04 2024 10:49:07 GMT+0000 (Coordinated Universal Time)",
		"url":"/Blogging/study/web/HTTPS/",
		"content": "",
		"tags": [ "note"]
},

{
		"title": "IP",
		"date":"Thu Apr 04 2024 10:49:07 GMT+0000 (Coordinated Universal Time)",
		"url":"/Blogging/study/web/IP/",
		"content": "",
		"tags": [ "note"]
},

{
		"title": "ISP",
		"date":"Thu Apr 04 2024 10:49:07 GMT+0000 (Coordinated Universal Time)",
		"url":"/Blogging/study/web/ISP/",
		"content": "",
		"tags": [ "note"]
},

{
		"title": "Router",
		"date":"Thu Apr 04 2024 10:49:07 GMT+0000 (Coordinated Universal Time)",
		"url":"/Blogging/study/web/Router/",
		"content": "",
		"tags": [ "note"]
},

{
		"title": "TCP",
		"date":"Thu Apr 04 2024 10:49:07 GMT+0000 (Coordinated Universal Time)",
		"url":"/Blogging/study/web/TCP/",
		"content": "",
		"tags": [ "note"]
},

{
		"title": "URL",
		"date":"Thu Apr 04 2024 10:49:07 GMT+0000 (Coordinated Universal Time)",
		"url":"/Blogging/study/web/URL/",
		"content": "",
		"tags": [ "note"]
},

{
		"title": "브라우저에 URL를 넣었을 때 어찌 되는가?",
		"date":"Thu Apr 04 2024 10:49:07 GMT+0000 (Coordinated Universal Time)",
		"url":"/Blogging/study/web/What is done when you enter a URL in your browser/",
		"content": "다음과 같은 순서로 진행 됩니다.\n\n브라우저에 <a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/Blogging/study/web/URL/\">URL</a> 주소를 넣습니다.\n브라우저에서 <a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/Blogging/study/web/URL/\">URL</a>를 해석합니다.\n\n<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/Blogging/study/web/URL/\">URL</a> 문법이 맞지 않다면 기본 검색 엔진으로 검색\n<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/Blogging/study/web/URL/\">URL</a> 문법이 맞다면, <a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/Blogging/study/web/URL/\">URL</a>의 호스트 부분을 인코딩합니다.\n\n<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/Blogging/study/web/HSTS/\">HSTS</a> 확인하고 있으면 <a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/Blogging/study/web/HTTPS/\">HTTPS</a>로 요청합니다.\n없으면 <a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/Blogging/study/web/HTTP/\">HTTP</a>로 요청합니다.\n\n브라우저는 캐싱 된 <a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/Blogging/study/web/DNS/\">DNS</a> 기록들에서 해당 <a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/Blogging/study/web/URL/\">URL</a>에 대응되는 <a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/Blogging/study/web/IP/\">IP</a> 주소가 있는지 확인합니다.\n\n<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/Blogging/study/web/DNS/\">DNS</a>의 목적은 편의성 제공이며, 캐싱은 네트워크 트래픽을 조절하고 데이터 전송 시간을 줄입니다.\nDNS query로 먼저 브라우저 <a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/Blogging/study/web/Cache/\">Cache</a>를 확인합니다.\n없다면 OS <a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/Blogging/study/web/Cache/\">Cache</a>를 확인합니다.\n없다면 <a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/Blogging/study/web/Router/\">Router</a> <a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/Blogging/study/web/Cache/\">Cache</a>를 확인합니다.\n없다면 <a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/Blogging/study/web/ISP/\">ISP</a> <a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/Blogging/study/web/Cache/\">Cache</a>를 확인합니다.\n\n요청한 <a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/Blogging/study/web/URL/\">URL</a>이 <a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/Blogging/study/web/Cache/\">Cache</a>에 없다면 <a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/Blogging/study/web/ISP/\">ISP</a>의 <a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/Blogging/study/web/DNS/\">DNS</a>서버가 해당 <a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/Blogging/study/web/Domain/\">Domain</a> 호스팅하고 있는 서버의 <a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/Blogging/study/web/IP/\">IP</a> 주소를 찾기 위해 DNS query를 ??에 보냅니다.\n브라우저가 서버와 <a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/Blogging/study/web/TCP/\">TCP</a> 커넥션을 합니다.\n브라우저가 웹 서버에 <a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/Blogging/study/web/HTTP/\">HTTP</a> 요청을 합니다.\n서버가 요청을 처리하고 <a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/Blogging/study/web/response/\">response</a>를 생성합니다.\n서버가 HTTP response를 보냅니다.\n브라우저가 받은 response의 body부분으로 content를 보여줍니다.\n\nDOM tree 생성을 합니다.\nCSSOM tree를 생성합니다.\n렌더링 tree 생성합니다.\nJavascript 파싱과 실행을 합니다.\n레이아웃을 그립니다.\n페이팅을 합니다.\n\n<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/Blogging/study/web/reflow/\">reflow</a>\n<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/Blogging/study/web/repaint/\">repaint</a>\n\n#web #browser #interview #frontend",
		"tags": ["web", "browser", "interview", "frontend", "note"]
},

{
		"title": "reflow",
		"date":"Thu Apr 04 2024 10:49:07 GMT+0000 (Coordinated Universal Time)",
		"url":"/Blogging/study/web/reflow/",
		"content": "",
		"tags": [ "note"]
},

{
		"title": "repaint",
		"date":"Thu Apr 04 2024 10:49:07 GMT+0000 (Coordinated Universal Time)",
		"url":"/Blogging/study/web/repaint/",
		"content": "",
		"tags": [ "note"]
},

{
		"title": "request",
		"date":"Thu Apr 04 2024 10:49:07 GMT+0000 (Coordinated Universal Time)",
		"url":"/Blogging/study/web/request/",
		"content": "",
		"tags": [ "note"]
},

{
		"title": "response",
		"date":"Thu Apr 04 2024 10:49:07 GMT+0000 (Coordinated Universal Time)",
		"url":"/Blogging/study/web/response/",
		"content": "",
		"tags": [ "note"]
},

{
		"title": "Welcome",
		"date":"Thu Apr 04 2024 10:49:07 GMT+0000 (Coordinated Universal Time)",
		"url":"/",
		"content": "이곳은 공부를 한 것에 대해서 정리하거나 생각을 정리하는 곳입니다.\n아직은 빈 페이지가 많지만 열심히 채울 예정입니다.",
		"tags": [ "note","gardenEntry"]
}
]
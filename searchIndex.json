[
{
		"title": "[24.02.04] 일상",
		"date":"Thu Apr 04 2024 10:49:08 GMT+0000 (Coordinated Universal Time)",
		"url":"/Blogging/daily/24-02-04/",
		"content": "간만에 브라우니70 본점에 와서 스터디 하는 중.\n공부 할 것이 많다...\n#daily #일상",
		"tags": ["daily", "일상", "note"]
},

{
		"title": "24-02-08",
		"date":"Thu Apr 04 2024 10:49:08 GMT+0000 (Coordinated Universal Time)",
		"url":"/Blogging/daily/24-02-08/",
		"content": "새해 복 많이 받으세요.",
		"tags": [ "note"]
},

{
		"title": "1장 GPT-4, ChatGPT, 랭체인 개요",
		"date":"Thu Apr 04 2024 10:49:08 GMT+0000 (Coordinated Universal Time)",
		"url":"/Blogging/study/books/GPT-4+ChatGPT+라마인덱스+랭체인을 활용한 인공지능 프로그래밍/1장/",
		"content": "GPT-4와 ChatGPT 및 랭체인\nChatGPT란?\nGPT-4와 GPT-3.5란?\n대규모 언어 모델이란?\nOpenAI API란?\n라마인덱스란?\n랭체인이란?\n대규모 언어 모델의 활용 사례\n인공지능과 머신러닝 및 딥러닝\n#chatgpt #langchain #랭체인 #라마인덱스 #인공지능 #머신러닝 #딥러닝",
		"tags": ["chatgpt", "langchain", "랭체인", "라마인덱스", "인공지능", "머신러닝", "딥러닝", "note"]
},

{
		"title": "6장 랭체인(LangChain)",
		"date":"Thu Apr 04 2024 10:49:08 GMT+0000 (Coordinated Universal Time)",
		"url":"/Blogging/study/books/GPT-4+ChatGPT+라마인덱스+랭체인을 활용한 인공지능 프로그래밍/2장/",
		"content": "랭체인이란?\n\n언어 모델을 기반으로 하는 애플리케이션을 개발하기 위한 프레임워크\nLangChain 라이브러리\n\nPython 및 JavaScript 라이브러리. 수많은 구성 요소에 대한 인터페이스 및 통합, 이러한 구성 요소를 체인 및 에이전트로 결합하기 위한 기본 런타임, 체인 및 에이전트의 구성\n\n서비스 구조\n\nLangChain 템플릿\n\n다양한 작업을 위해 쉽게 배포할 수 있는 참조 아키텍처 모음\n\nLangServe\n\nLangChain 체인을 REST API로 배포하기 위한 라이브러리\n\nLangSmith\n\nLLM 프레임워크에 구축된 체인을 디버그, 테스트, 평가 및 모니터링하고 LangChain과 원활하게 통합할 수 있는 개발자 플랫폼\n\n개발\n\nLangChain/LangChain.js에 애플리케이션을 작성\n\n생산화\n\nLangSmith를 사용하여 체인을 검사, 테스트 및 모니터링\n\n배포\n\nLangServe를 사용하여 모든 체인을 API로 전환\n\n모듈\n\nLLM\n\nLLM 호출을 위한 공통 인터페이스\n\n프롬프트 템플릿\n\n사용자 입력에 따른 프롬프트 생성\n\n체인\n\n여러 LLM과 프롬프트의 입출력을 연결\n\n에이전트\n\n사용자의 요청에 따라 어떤 기능을 어떤 순서로 실행할 것인지 결정\n\n도구\n\n에이전트가 수행하는 특정 기능\n\n메모리\n\n체인 및 에이전트의 메모리 보유\n\n설치\npip install langchain\n\nLLM 예제\n\nLLM 호출을 위한 공통 인터페이스\n\n!pip install langchain\n!pip install openai\n\nimport os\nos.environ[&quot;OPENAI_API_KEY&quot;] = &quot;APIKEY&quot;\n\nfrom langchain.llms import OpenAI\nllm = OpenAI(temperature=0.9)\n\nprint(llm(&quot;컴퓨터 게임을 만드는 새로운 한국어 회사명을 하나 제안해 주세요.&quot;); //비바게임즈\n\n프롬프트 템플릿 예제\n\n프롬프트 템플릿은 사용자 입력으로 프롬프트를 생성하기 위한 템플릿\n\nfrom langchain.prompts import PromptTemplate\nfrom langchain.chains import LLMChain\n\nprompt = PromptTemplate(\n\tinput_variables=[&quot;product&quot;],\n\ttemplate=&quot;{product}을 만드는 새로운 한국어 회사명을 하나 제안해 주세요.&quot;,\n)\n\nprint(prompt.format(product=&quot;가정용 로봇&quot;))\n\n체인 사용법\n\n여러 개의 LLM이나 프롬프트의 입출력을 연결하기 위한 모듈\n\nfrom langchain.chains import LLMChain\nfrom langchain.llm import OepnAI\nfrom langchin.prompts import PromptTemplate\n\nprompt = PromptTemplate(\n\tinput_variables=[&quot;product&quot;],\n\ttemplate=&quot;{product}을 만드는 새로운 한국어 회사명을 하나 제안해 주세요.&quot;,\n)\n\nchain = LLMChain(\n\tllm=OpenAI(temperature=0.9),\n\tprompt=prompt\n)\n\n# 싱글 input, output일때는 run을 쓸수 있다.\nchain.run(&quot;가정용 로봇&quot;)\n\n# 여러 input, output일때는 call을 쓴다.\nchain.call({product:&quot;가정용 로봇&quot;})\n\n에이전트와 도구 사용법\n\n사용자의 요청에 따라 어떤 기능을 어떤 순서로 실행할지 결정하는 모듈\n체인은 미리 정해져 있는 기능을 실행하는 반면, 에이전트는 사용자의 요청에 따라 수해오디는 기능\n예시로 웹 검색과 수치 계산을 진행합니다.\n\nSerpAPI, llm-math\n\n!pip install google-search-results\n\nimport os\nos.environ[&quot;SERPAPI_API_KEY&quot;] = &quot;SERPAPI_KEY&quot;\n\nfrom langchain.agents import load_tools\nfrom langchain.llms import OpenAI\n\ntools = load_tools(\n\ttools_names=[&quot;serpapi&quot;,&quot;llm-math&quot;],\n\tllm-OpenAI(temperature=0)\n)\n\nfrom langchain.agents import initialize_agent\n\nagent = initialize_agent(\n\tagent=&quot;zero-shot-react-description&quot;,\n\tllm=OpenAI(temperature=0),\n\ttools=tools,\n\tverbose=True\n)\n\n# llm이 질문을 판단하여 llm-math를 사용합니다.\nagnet.run(&quot;123*4를 계산기로 계산하세요.&quot;)\n\n# llm이 질문을 판단하여 SerpAPI를 사용합니다.\nagent.run(&quot;오늘 한국 서울 날시를 웹 검색으로 확인하세요.&quot;);\n\n메모리 사용 예제\n\n체인이나 에이턴트의 과거 기억을 보관하는 모듈\n\n이전 대화를 기억하고 그 정보를 현재 대화에 반영\n\nfrom langchain.chains import ConversationChain\nfrom langchain.llms import OpenAI\n\nchain = ConversationChain(\n\tllm=OpenAI(temperature=0),\n\tverbose=True\n)\n\nUse cases\nhttps://python.langchain.com/docs/use_cases\nLangServe\n\n개발자가 LangChain 실행 가능 파일과 체인을 REST API로 배포하는 데 도움\nLangChain 애플리케이션의 원클릭 배포를 위해 LangServe의 호스팅 버전을 출시할 예정\n\n특징\n\n사용자의 LangChain 개체에서 자동으로 추론된 입력 및 출력 스키마는 API 호출마다 강제되며, 풍부한 오류 메시지와 함께 사용됩니다.\nJSONSchema 및 Swagger가 포함된 API 문서 페이지 (예시 링크 삽입)\n단일 서버에서 많은 동시 요청을 지원하는 효율적인 /invoke/, /batch/ 및 /stream/ 엔드포인트\n체인/에이전트에서 모든 (또는 일부) 중간 단계를 스트리밍하는 /stream_log/ 엔드포인트\n0.0.40 버전부터는 astream_events를 지원하여 stream_log의 출력을 구문 분석할 필요 없이 쉽게 스트리밍할 수 있습니다.\n스트리밍 출력과 중간 단계를 제공하는 Playground 페이지 (/playground/)\nLangSmith에 내장된 (옵션) 추적 기능, API 키를 추가하기만 하면 됩니다.\nFastAPI, Pydantic, uvloop 및 asyncio와 같은 검증된 오픈 소스 Python 라이브러리를 사용하여 모두 구축되었습니다.\n클라이언트 SDK를 사용하여 LangServe 서버를 로컬에서 실행 중인 Runnable처럼 호출하거나 HTTP API를 직접 호출할 수 있습니다.\nLangServe Hub\n\n설치\npip install &quot;langserve[all]&quot;\n\n예제\n\nOpenAI 및 Anthropic 채팅 모델을 예약하는 LLM 최소 예입니다. 비동기를 사용하고 일괄 처리 및 스트리밍을 지원합니다.\n\n# Server\n&quot;&quot;&quot;Example LangChain server exposes multiple runnables (LLMs in this case).&quot;&quot;&quot;\n\nfrom fastapi import FastAPI\nfrom langchain.chat_models import ChatAnthropic, ChatOpenAI\n\nfrom langserve import add_routes\n\napp = FastAPI(\ntitle=&quot;LangChain Server&quot;,\nversion=&quot;1.0&quot;,\ndescription=&quot;Spin up a simple api server using Langchain's Runnable interfaces&quot;,\n)\n\nadd_routes(\napp,\nChatOpenAI(),\npath=&quot;/openai&quot;,\n)\nadd_routes(\napp,\nChatAnthropic(),\npath=&quot;/anthropic&quot;,\n)\n\nif __name__ == &quot;__main__&quot;:\nimport uvicorn\n\nuvicorn.run(app, host=&quot;localhost&quot;, port=8000)\n\n# Client\nfrom langchain.prompts.chat import ChatPromptTemplate\nfrom langserve import RemoteRunnable\n\nopenai_llm = RemoteRunnable(&quot;&lt;http://localhost:8000/openai/&gt;&quot;)\nanthropic = RemoteRunnable(&quot;&lt;http://localhost:8000/anthropic/&gt;&quot;)\n\n# We can use either LLM\nprompt = ChatPromptTemplate.from_messages(\n[\n(\n&quot;system&quot;,\n&quot;You are a highly educated person who loves to use big words. &quot;\n+ &quot;You are also concise. Never answer in more than three sentences.&quot;,\n),\n(&quot;human&quot;, &quot;Tell me about your favorite novel&quot;),\n]\n).format_messages()\n\n# AIMessage(content=&quot; My favorite novel is Moby Dick by Herman Melville. The intricate plot and rich symbolism make it a complex and rewarding read. Melville's masterful prose vividly evokes the perilous life of whalers on 19th century ships.&quot;, additional_kwargs={}, example=False)\nanthropic.invoke(prompt)\n\n# My favorite novel is Moby-Dick by Herman Melville. The epic tale of Captain Ahab's quest to find and destroy the great white whale is a masterwork of American literature. Melville's dense, philosophical prose and digressive storytelling style make the novel a uniquely challenging and rewarding read.\nfor chunk in anthropic.stream(prompt):\nprint(chunk.content, end=&quot;&quot;, flush=True)\n\n# My favorite novel is The Art of Language by Maximo Quilana. It is a philosophical treatise on the beauty and complexity of human speech. The prose is elegant yet precise.\nasync for chunk in anthropic.astream(prompt):\nprint(chunk.content, end=&quot;&quot;, flush=True)\n\n# As with regular runnables, async invoke, batch and async batch variants are available by default\nopenai_llm.invoke(prompt)\n# AIMessage(content='My favorite novel is &quot;Ulysses&quot; by James Joyce. It\\\\'s a complex and innovative work that explores the intricacies of human consciousness and the challenges of modernity in a highly poetic and experimental manner. The prose is richly layered and rewards careful reading.', additional_kwargs={}, example=False)\nawait openai_llm.ainvoke(prompt)\n\n#[AIMessage(content=&quot; My favorite novel is Moby Dick by Herman Melville. The epic tale of Captain Ahab's obsessive quest to kill the great white whale is a profound meditation on man's struggle against nature. Melville's poetic language immerses the reader in the mysticism of the high seas.&quot;, additional_kwargs={}, example=False),\n# AIMessage(content=&quot; My favorite novel is Moby Dick by Herman Melville. The intricate details of whaling, though tedious at times, serve to heighten the symbolism and tension leading to the epic battle between Captain Ahab and the elusive white whale. Melville's sublime yet economical prose immerses the reader in a turbulent seascape teeming with meaning.&quot;, additional_kwargs={}, example=False)]\nanthropic.batch([prompt, prompt])\n\n# Streaming is available by default\n\n# [AIMessage(content=' Here is a concise description of my favorite novel in three sentences:\\\\n\\\\nMy favorite novel is Moby Dick by Herman Melville. It is the epic saga of the obsessed Captain Ahab pursuing the white whale that crippled him through the seas. The novel explores deep philosophical questions through rich symbols and metaphors.', additional_kwargs={}, example=False),\n# AIMessage(content=&quot; My favorite novel is Moby Dick by Herman Melville. The epic tale of Captain Ahab's obsessive quest for the great white whale is a masterpiece of American literature. Melville's writing beautifully evokes the mystery and danger of the high seas.&quot;, additional_kwargs={}, example=False)]\nawait anthropic.abatch([prompt, prompt])\n\nfrom langchain.schema.runnable import RunnablePassthrough\ncomedian_chain = (\nChatPromptTemplate.from_messages(\n[\n(\n&quot;system&quot;,\n&quot;You are a comedian that sometimes tells funny jokes and other times you just state facts that are not funny. Please either tell a joke or state fact now but only output one.&quot;,\n),\n]\n)\n| openai_llm\n)\n\njoke_classifier_chain = (\nChatPromptTemplate.from_messages(\n[\n(\n&quot;system&quot;,\n&quot;Please determine if the joke is funny. Say `funny` if it's funny and `not funny` if not funny. Then repeat the first five words of the joke for reference...&quot;,\n),\n(&quot;human&quot;, &quot;{joke}&quot;),\n]\n)\n| anthropic\n)\n\nchain = {&quot;joke&quot;: comedian_chain} | RunnablePassthrough.assign(\nclassification=joke_classifier_chain\n)\n\n# {'joke': AIMessage(content=&quot;Why don't scientists trust atoms?\\\\n\\\\nBecause they make up everything!&quot;, additional_kwargs={}, example=False),\n# 'classification': AIMessage(content=&quot; not funny\\\\nWhy don't scientists trust atoms?&quot;, additional_kwargs={}, example=False)}\nchain.invoke({})\n\nLangSmith\n\n프로덕션급 LLM 애플리케이션을 구축하기 위한 플랫폼\n새로운 체인, 에이전트 또는 도구 세트를 신속하게 디버깅\n미세 조정, 몇 번의 메시지 표시 및 평가를 위한 데이터 세트 생성 및 관리\n자신 있게 개발하려면 애플리케이션에서 회귀 테스트를 실행하세요.\n제품 통찰력과 지속적인 개선을 위한 생산 분석 캡처\n\nLangSmith와 Lilac으로 LLM을 fine-tuning\nhttps://blog.langchain.dev/fine-tune-your-llms-with-langsmith-and-lilac/\nOpenAI Fine-Tuning\nfrom langsmith import Client\n\nclient = Client()\n\nimport datetime\n\nproject_name = &quot;default&quot;\nrun_type = &quot;llm&quot;\nend_time = datetime.datetime.now()\n\nruns = client.list_runs(\nproject_name=project_name,\nrun_type=run_type,\nerror=False,\n)\n\nfrom langchain import chains, chat_models, prompts, schema, callbacks\n\nchain = prompts.ChatPromptTemplate.from_template(&quot;Tell a joke for:\\\\n{input}&quot;) | chat_models.ChatAnthropic(tags=['my-anthropic-run']) | schema.output_parser.StrOutputParser()\n\nwith callbacks.collect_runs() as cb:\nchain.invoke({&quot;input&quot;: &quot;foo&quot;})\n# Assume feedback is logged\nrun = cb.traced_runs[0]\nclient.create_feedback(run.id, key=&quot;user_click&quot;, score=1)\n\nproject_name = &quot;default&quot;\nend_time = datetime.datetime.now()\n\nruns = client.list_runs(\nproject_name=project_name,\nexecution_order=1,\nfilter='and(eq(feedback_key, &quot;user_click&quot;), eq(feedback_score, 1))',\n# For continuous scores, you can filter for &gt;, &lt;, &gt;=, &lt;= with the followingg arguments: gt/lt/gte/lte(feedback_score, 0.9)\n# filter='and(eq(feedback_key, &quot;user_click&quot;), gt(feedback_score, 0.9))',\nerror=False,\n)\n\nllm_runs = []\nfor run in runs:\nllm_run = next(client.list_runs(project_name=project_name, run_type=&quot;llm&quot;, parent_run_id=run.id))\nllm_runs.append(llm_run)\n\nllm_runs[0].tags\n\n# For any &quot;Chain&quot; object, you can add tags directly on the Example with LLMChain\nimport uuid\n\nunique_tag = f&quot;call:{uuid.uuid4()}&quot;\n\nchain = chains.LLMChain(\nllm=chat_models.ChatAnthropic(tags=['my-cool-llm-tag']), # This tag will only be applied to the LLM\nprompt=prompts.ChatPromptTemplate.from_template(&quot;Tell a joke based on the following prompt:\\\\n\\\\nPrompt:{input}&quot;),\ntags=[&quot;my-tag&quot;]\n)\n\n# You can also define at call time for the call/invoke/batch methods.\n# This tag will be propagated to all child calls\nprint(chain({&quot;input&quot;: &quot;podcasting these days&quot;}, tags=[unique_tag]))\n\n# If you're defining using Runnables (aka langchain expression language)\nrunnable = (\nprompts.ChatPromptTemplate.from_template(&quot;Tell a joke based on the following prompt:\\\\n\\\\nPrompt:{input}&quot;)\n| chat_models.ChatAnthropic(tags=['my-cool-llm-tag']) # This tag will only be applied to the LLM\n| schema.StrOutputParser(tags=['some-parser-tag'])\n)\n\n# Again, you can tag at call time as well. This tag will be propagated to all child calls\nprint(runnable.invoke({&quot;input&quot;: &quot;podcasting these days&quot;}, {&quot;tags&quot;: [unique_tag]}))\n\nproject_name = &quot;default&quot;\nend_time = datetime.datetime.now()\n\nruns = client.list_runs(\nexecution_order=1, # Only return the root trace\nfilter=f'has(tags, &quot;{unique_tag}&quot;)',\n)\nlen(list(runs))\n\nproject_name = &quot;default&quot;\nrun_type = &quot;llm&quot;\nend_time = datetime.datetime.now()\n\nruns = client.list_runs(\nproject_name=project_name,\nrun_type=run_type,\nfilter='eq(name, &quot;ChatAnthropic&quot;)',\nerror=False,\n)\n\n# Example chain for the following query\nfrom langchain import prompts, chat_models\n\nchain = (\nprompts.ChatPromptTemplate.from_template(\n&quot;Summarize the following chat log: {input}&quot;\n)\n| chat_models.ChatOpenAI()\n)\n\nchain.invoke({&quot;input&quot;: &quot;hi there, hello....&quot;})\n\nimport datetime\n\nproject_name = &quot;default&quot;\nrun_type = &quot;prompt&quot;\nend_time = datetime.datetime.now()\n\nruns = client.list_runs(\nproject_name=project_name,\nrun_type=run_type,\nend_time=end_time,\nerror=False,\n)\n\n# You can then get a sibling LLM run by searching by parent_run_id and including other criteria\nfor prompt_run in runs:\nllm_run = next(client.list_runs(project_name=project_name, run_type=&quot;llm&quot;, parent_run_id=prompt_run.parent_run_id))\ninputs, outputs = prompt_run.inputs, llm_run.outputs\n\ndataset = client.create_dataset(\ndataset_name = &quot;Fine-Tuning Dataset Example&quot;,\ndescription=f&quot;Chat logs taken from project {project_name} for fine-tuning&quot;,\ndata_type=&quot;chat&quot;,\n)\nfor run in runs:\nif 'messages' not in run.inputs or not run.outputs:\n# Filter out non chat runs\ncontinue\ntry:\n# Convenience method for creating a chat example\nclient.create_example_from_run(\ndataset_id=dataset.id,\nrun=run,\n)\n# Or if you want to select certain keys/values in inputs\n# inputs = convert_inputs(run.inputs)\n# outputs = convert_outputs(run.outputs)\n# client.create_example(\n# dataset_id=dataset.id,\n# inputs=inputs,\n# outputs=outputs,\n# run=run,\n# )\nexcept:\n# Duplicate inputs raise an exception\npass\n\nfrom langsmith import schemas\nfrom langchain import load\n\ndef convert_messages(example: schemas.Example) -&gt; dict:\nmessages = load.load(example.inputs)['messages']\nmessage_chunk = load.load(example.outputs)['generations'][0]['message']\nreturn {&quot;messages&quot;: messages + [message_chunk]}\n\nmessages = [\nconvert_messages(example)\nfor example in client.list_examples(dataset_name=&quot;Fine-Tuning Dataset Example&quot;)\n]\n\nfrom langchain.adapters import openai as openai_adapter\n\nfinetuning_messages = openai_adapter.convert_messages_for_finetuning(messages)\n\nimport time\nimport json\nimport io\n\nimport openai\n\nmy_file = io.BytesIO()\nfor group in finetuning_messages:\nif any([&quot;function_call&quot; in message for message in group]):\ncontinue\nmy_file.write((json.dumps({&quot;messages&quot;: group}) + &quot;\\\\n&quot;).encode('utf-8'))\n\nmy_file.seek(0)\ntraining_file = openai.File.create(\nfile=my_file,\npurpose='fine-tune'\n)\n\n# Wait while the file is processed\nstatus = openai.File.retrieve(training_file.id).status\nstart_time = time.time()\nwhile status != &quot;processed&quot;:\nprint(f&quot;Status=[{status}]... {time.time() - start_time:.2f}s&quot;, end=&quot;\\\\r&quot;, flush=True)\ntime.sleep(5)\nstatus = openai.File.retrieve(training_file.id).status\nprint(f&quot;File {training_file.id} ready after {time.time() - start_time:.2f} seconds.&quot;)\n\njob = openai.FineTuningJob.create(\ntraining_file=training_file.id,\nmodel=&quot;gpt-3.5-turbo&quot;,\n)\n\n# It may take 10-20+ minutes to complete training.\nstatus = openai.FineTuningJob.retrieve(job.id).status\nstart_time = time.time()\nwhile status != &quot;succeeded&quot;:\nprint(f&quot;Status=[{status}]... {time.time() - start_time:.2f}s&quot;, end=&quot;\\\\r&quot;, flush=True)\ntime.sleep(5)\njob = openai.FineTuningJob.retrieve(job.id)\nstatus = job.status\n\nfrom langchain import chat_models, prompts\n\nmodel_name = job.fine_tuned_model\n# Example: ft:gpt-3.5-turbo-0613:personal::5mty86jblapsed\nmodel = chat_models.ChatOpenAI(model=model_name)\nchain.invoke({&quot;input&quot;: &quot;Who are you designed to assist?&quot;})\n\nLangGraph\n\n구축된(그리고 함께 사용하도록 의도된) LLM을 사용하여 상태 저장 다중 행위자 애플리케이션을 구축하기 위한 라이브러리\n주요 용도는 LLM 어플리케이션에 사이클을 추가하는 것입니다.\n\n#langchain #랭체인 #chatgpt",
		"tags": ["langchain", "랭체인", "chatgpt", "note"]
},

{
		"title": "1장 텍스트 마이닝 기초",
		"date":"Thu Apr 04 2024 10:49:08 GMT+0000 (Coordinated Universal Time)",
		"url":"/Blogging/study/books/파이썬 텍스트 마이닝 완벽 가이드/1장/",
		"content": "1장 텍스트 마이닝 기초\n1.1. 텍스트 마이닝의 정의\n\n무엇인가?\n\n텍스트에서 고품질 정보를 추출하는 과정\n이렇게 추출한 대규모 데이터에서 패턴을 찾는 것.\n\n패턴이란?\n\n예시) 어떤 물건을 선택한 고객이 다른 물건을 함께 구매할 확률을 알아내서 그에 따른 쿠폰이나 상품을 보여주는 것\n\n어떻게?\n\n통계적 패턴 학습\n\n머신러닝이 이에 해당\n\n다만 머신러닝은 정형화된 데이터가 필요하기에 비정형화된 데이터를 정형화 하는 작업이 필요\n\n정리\n\n자연어 처리 기법을 이용해 텍스트를 정형화된 데이터로 변환하고, 머신러닝 기법을 적용해 우리가 관심이 있는 어떤 사건을 예측하고자 하는 방법론\n\n정형화된 데이터의 형태는 일정 길이의 벡터\n\n이렇게 일정 길이의 벡터로 변환 하는 것을 **임베딩**이라고 함\n\n1.2. 텍스트 마이닝 트렌드 변화\n\n여기서는 어떤 것이 있는지만 얘기. 자세한 것은 다른 장에서 다룰 예정\n카운트 기반 문서 표현\n\n딥러닝 전 얘기이며, 문장에 있는 단어들의 개수를 세고, 주로 사용된 단어들을 이용해 그 문장의 내용을 파악하는것\n단어들의 빈도를 이용한 하나의 벡터로 단번에 문서 전체를 표현\n벡터화\n\n단어 : [A, B, C, D, E]\n횟수 : [3, 7, 4, 2, 5]\n\n단점으로는 사용된 단어가 많으면 벡터의 크기가 많이 커진다.\n\n시퀀스 기반의 문서 표현\n\n카운트 기반 문서 표현의 문제점을 해결\n사람이 글을 읽고 이해하는 것과 유사한 방법으로 텍스트의 문맥을 이해하고자 하는 방식\n각 단어를 먼저 벡터화 하고, 벡터의 연속된 나열 혹은 시퀀스로 문서 표현\n원 핫 인코딩 : 단어를 일정 규칙에 따라 정렬하고 단어의 수만큼의 벡터를 만들고 단어 자신의 위치만 1로 표시 하는것.\n예시\n\n단어 : [A, B, C, D, E]\nA : [1, 0, 0, 0, 0]\nB : [0, 1, 0, 0, 0]\nAB: [[1, 0, 0, 0, 0],[0, 1, 0, 0, 0]]\n\n단점\n\n단어를 벡터로 변환 했을때 벡터가 지나치게 커진다.\n문서 또는 문장이 가변의 길이인 단어로 되어 있기에 벡터의 시퀀스 길이가 제각각임\n\n머신러닝이나 딥러닝은 가변 길이의 입력을 보통은 허용하지 않으며, 일정 길이를 기준으로 짧은것은 null이나 0으로 채우고 길면 짤라서 사용한다.\n\n용어 정리\n\n텍스트, 문서, 문장\n\n텍스트, 문서 : 하나의 일관된 목적 혹은 주제를 가지고 쓰여진 글\n문장 : 생각이나 감정을 말로 표현할때 완결된 내용을 나타내는 최소 단위\n\n말뭉치 : 언어 연구를 위해 컴퓨터가 텍스트를 가공, 처리, 분석 할 수 있는 형태로 모아놓은 자료의 집합 or 자연언어 연구를 위해 특정한 목적을 가지고 언어의 표본을 추출한 집단\n\n1.3. 텍스트 마이닝에 필요한 지식과 도구\n\n자연어 처리 기법\n\n컴퓨터를 이용해 사람의 자연어를 분석하고 처리하는 기술\n위키에서는 아래와 같이 처리 기법을 얘기함\n\n형태소 분석\n품사 부착\n구절 단위 분석\n구문 분석\n\n이 책에서는 텍스트 전처리를 위한 기법을 아래와 같이 얘기함\n\n토큰화 (tokenize)\n어간 추출 (stemming)\n표제어 추출(lemmatize)\n정규화 (Normalization)\n품사 태킹 (POS-tagging)\n\n라이브러리\n\nNLTK (Natural Langauge Tool-kit)\nKoNLPy\n\n통계학과 선형대수\n\n텍스트로부터 고품질의 정보를 추출하는데 다양한 통계적 분석 방법 쓰임\n회귀분석, SVM과 같은 머신러닝 방법론을 이해하기 위해서는 통계학이 필수\n대용량의 데이터를 다루려면 행렬도 잘 알아야함.\n라이브러리\n\nNumpy\nPandas\n\n시각화 기법\n\n전달하고자 하는 내용을 한눈에 쉽게 이해시키려고 사용\n텍스트 마이닝에서는 막대그래프와 워드클라우드와 같은 기법을 많이 사용\n라이브러리\n\nmatplotlib\nseaborn\n\n머신러닝\n\n공통적인 알고리즘을 데이터에 적용해 주어진 데이터에 적합한 문제해결 방안을 생성하는 방식\n기계학습\n\n지도학습\n\n회귀\n분류\n\n비지도 학습\n\n클러스터링\n차원축소\n\n강화학습\n\n라이브러리\n\n사이킷런\n\n딥러닝\n\n머신러닝의 한 분류에 속하는 인공신경망에서 은닉층을 깊게 쌓은 신경망 구조를 활용해 학습하는 알고리즘\n이미지 인식, 자연어 처리, 음성 처리 등에서 뛰어난 결과를 보이고 있음\n매우 어려움\n초기에는 RNN, LSTM, CNN등 단순하고 쉬운 방법론이 있었지만\n트랜스포머(Transformer)가 등장하면서 BERT, GPT와 같은 복잡한 기술이 등장\n누군가가 미리 학습된 모형을 가져와서 사용하기에 학습 부담이 많이 줄어듬\n\n전체 학습을 하지 않아도 되어서 시간도 절약됨\n기본 학습 모델에 나의 데이터를 이용해 미세조정(Fint-tunning)을 수행하면 자원 소모가 줄어든다.\n\n라이브러리\n\nKeras\nPytorch\nhuggingface에서 제공하는 SDK\n\n이건 책에 없지만 더 간단하게 허깅페이스에 등록된 모델을 쉽게 가져와 쓰기 좋아서 쓸 예정\n\n1.4. 텍스트 마이닝의 주요 적용 분야\n\n문서 분류\n문서 생성\n문서요약\n질의응답\n기계번역\n토픽 모델링\n\n1.5. 이 책의 실습 환경과 사용 소프트웨어\n\nwin 10\npython 3.8\nNLTK\nscikit-learn\nNumpy\nPandas\nAnaconda\nKoNLPy\nTextBlob\nAFINN\nVADER\nGensim\nTensorflow\nPytorch",
		"tags": [ "note"]
},

{
		"title": "2장 텍스트 전처리",
		"date":"Thu Apr 04 2024 10:49:08 GMT+0000 (Coordinated Universal Time)",
		"url":"/Blogging/study/books/파이썬 텍스트 마이닝 완벽 가이드/2장/",
		"content": "2.1. 텍스트 전처리 방식\n\n여기서는 대부분의 자연어 처리에서 공통적으로 수행되는 전처리에 대해서 설명한다.\n주어진 텍스트에서 노이즈와 같이 불필요한 부분을 제거하고, 문장을 표준 단어들로 분리한 후에 각 단어의 품사를 파악하는 것\n왜 필요한가?\n\n우리는 일반적으로 하나의 문장을 이해할 때는 사용된 단어들의 순차수열로 이해\n\n각각의 단어를 이해하고 그 단어들의 순서에 따라 의미를 이해\n\n컴퓨터에게 이해 시키려면 문장을 단어 단위로 나눈 후에 이 단어들의 리스트 형태로 변환 해주어야 한다.\n\n이때 쓸모없다고 생각되는 문자를 없엔다.\n\n예시 (별 헤는 밤)\n\n계절이 지나가는 하늘에는 가을로 가득 차 있습니다.\n[’계절이’, ‘지나가는’, ‘하늘에는’, ‘가을로’, ‘가득’, ‘차’, ‘있습니다’]\n마지막 ‘있습니다’는 ‘있다’에서 파생 단어여서 ‘있다’로 변환해서 쓸때 효율적일수 있다.\n\n전처리 단계\n\n정제 : 분석에 불필요한 노이즈를 제러\n\n사전에 있는 유의미한 단어라 하더라도 분석에 별 도움이 안되는 단어를 제거 (불용어 제거 라고 한다.)\n\n불용어 리스트 : https://www.ranks.nl/stopwords/korean\n\n토큰화\n\n주어진 텍스트를 원하는 단위(토큰)로 나누는 작업\n\n정규화\n\n같은 의미를 가진 동일한 단어임에도 불구하고 다른 형태로 쓰여진 단어들을 통일시켜서 표준 단어로 만드는 작업\n\n예시 goes는 3인칭에 쓰이는 동사이지만 go와 동일하게 판단하고 go로 변환 시킨다.\n\n품사태깅\n\n문법적인 기능에 따라 분류하는 것\n\n2.2. 토큰화\n\nNLTK는 교육용으로 개발된 자연어 처리 및 문서 분셕용 파이썬 패키지\n자연어 처리를 지원하는 다양한 라이브러리와 말뭉치, 예제를 제공\n\n# 필요한 nltk 라이브러리를 다운로드\nimport nltk\nnltk.download('punkt')\nnltk.download('webtext')\nnltk.download('wordnet')\nnltk.download('stopwords')\nnltk.download('averaged_perceptron_tagger')\n\n문장 토큰화\n\n먼저 문장 토큰화를 해보자.\nnltk의 sent_tokenize를 사용한다.\n각 문장을 문자열로 갖는 리스트를 출력한다.\n\npara = &quot;Hello everyone. It's good to see you. Let's start our text mining class!&quot;\n\nfrom nltk.tokenize import sent_tokenize\n\n# 주어진 텍스트를 문장 단위로 토큰화. 주로 . ! ? 등을 이용\nprint(sent_tokenize(para))\n\n# ['Hello everyone.', &quot;It's good to see you.&quot;, &quot;Let's start our text mining class!&quot;]\n\n만약에 다른 언어에 대해 문장 토큰화를 하려면 아래와 같이 사전 학습된 모델을 지정후 실행해야한다.\n안타깝게도 nltk에는 한글에 대해 사전학습된 모델이 없다. 다만 문장은 마침표나 ?, !같은 것을 기준으로 학습되어 있어서 영어 모델로도 어느정도 가능하다.\n\npara_kor = &quot;안녕하세요, 여러분. 만나서 반갑습니다. 이제 텍스트마이닝 클래스를 시작해봅시다!&quot;\n\nprint(sent_tokenize(para_kor))\n# ['안녕하세요, 여러분.', '만나서 반갑습니다.', '이제 텍스트마이닝 클래스를 시작해봅시다!']\n\n단어 토큰화\n\nNLTK에서는 word_tokenize로 단어 토큰화를 할 수 있다.\n\nfrom nltk.tokenize import word_tokenize\n\n# 주어진 text를 word 단위로 tokenize함\nprint(word_tokenize(para))\n\n# ['Hello', 'everyone', '.', 'It', &quot;'s&quot;, 'good', 'to', 'see', 'you', '.', 'Let', &quot;'s&quot;, 'start', 'our', 'text', 'mining', 'class', '!']\n\n아래는 WordPunctTokenizer를 사용한 방법이다.\n\nfrom nltk.tokenize import WordPunctTokenizer\n\nprint(WordPunctTokenizer().tokenize(para))\n# ['Hello', 'everyone', '.', 'It', &quot;'&quot;, 's', 'good', 'to', 'see', 'you', '.', 'Let', &quot;'&quot;, 's', 'start', 'our', 'text', 'mining', 'class', '!']\n\n위 2개 차이점은 It 다음에 ‘s로 할 것이냐 ‘ , s로 나눌것이냐 차이이고 목적에 따라서 토크나이저를 잘 나눠 써야 한다.\n한글은 NLTK에서 단어로는 잘 안되며, 대신에 KoNLPy를 이용해서 한글 전처리를 한다.\n\n정규표현식을 이용한 토큰화\n\n라이브러리는 편하지만 세밀하기 토큰화하기 어렵기에 정규표현식을 이용해서 다양한 조건에 따라 토큰화 할수 있다.\n정규식을 익혀야 하는 문제가 있지만 요즘 chatGPT가 잘 만들어주니 잘 가져다 쓰자\n여기서는 정규식 예제라서 설명은 건너뛴다.\n\n노이즈와 불용어 제거\n\n특수문자와 같은 불필요한 문자들 혹은 노이즈가 삭제를 시킨다. (요즘 특수문자도 해석을 위해 삭제 안하는 경우도 있긴 하다.)\n토큰화 과정과 별도로 정규표현식을 이용한 치환을 통해 원하는 패턴의 노이즈를 제거할 수 있다.\n불용어는 리스트를 보고 지우는 경우가 있다.\n\nfrom nltk.corpus import stopwords\n# 일반적으로 분석대상이 아닌 단어들\nenglish_stops = set(stopwords.words('english'))\n# 반복이 되지 않도록 set으로 변환\ntext1 = &quot;Sorry, I couldn't go to movie yesterday.&quot;\ntokenizer = RegexpTokenizer(&quot;[\\\\\\\\w']+&quot;)\ntokens = tokenizer.tokenize(text1.lower())\n# word_tokenize로 토큰화\n# stopwords를 제외한 단어들만으로 list를 생성\nresult = [word for word in tokens if word not in english_stops]\nprint(result)\n# ['sorry', 'go', 'movie', 'yesterday']\n\n# can't를 추가하려면?\ntokenizer = RegexpTokenizer(&quot;[\\\\\\\\w]+&quot;)\nprint(tokenizer.tokenize(&quot;Sorry, I can't go there.&quot;))\n# ['Sorry', 'I', 'can', 't', 'go', 'there']\n\n# 모두 소문자로 바꾸고 '를 포함해 세글자 이상의 단어들만 골라내려면?\ntext1 = &quot;Sorry, I can't go there.&quot;\ntokenizer = RegexpTokenizer(&quot;[\\\\w']{3,}&quot;)\nprint(tokenizer.tokenize(text1.lower()))\n# ['sorry', &quot;can't&quot;, 'there']\n\n노이즈와 불용어 제거\n\n앞에서 정규표현식을 이용한 토큰화 과정을 보면 특수문자와 같은 불필요한 문자들 혹은 노이즈를 삭제한다.\n영어 같은 경우 보통 길이가 3미만인 단어들은 삭제를 한다.\n또는 stopworkd라는 라이브러리를 사용해서 불용어 사전을 참조해 삭제한다. (직접 만들수도 있고 가져와서 쓰기도 한다.)\n\nfrom nltk.corpus import stopwords\n# 일반적으로 분석대상이 아닌 단어들\nenglish_stops = set(stopwords.words('english'))\n# 반복이 되지 않도록 set으로 변환\ntext1 = &quot;Sorry, I couldn't go to movie yesterday.&quot;\ntokenizer = RegexpTokenizer(&quot;[\\\\\\\\w']+&quot;)\ntokens = tokenizer.tokenize(text1.lower())\n# word_tokenize로 토큰화\n# stopwords를 제외한 단어들만으로 list를 생성\nresult = [word for word in tokens if word not in english_stops]\nprint(result)\n# ['sorry', 'go', 'movie', 'yesterday']\n\n불용어 사전에 무엇이 있는지 알고 싶으면 아래와 같이 볼수 있다.\n\nprint(english_stops)\n# {'his', 'any', 'was', 'it', 'being', &quot;wasn't&quot;, 'its', 'down', 'has', 'again', 'itself', 'weren', 'did', &quot;you'll&quot;, 'into', 'than', 'wouldn', 'these', 'he', 'o', 'so', &quot;needn't&quot;, 'my', 'y', 'some', 'below', 'how', 'ourselves', 'hadn', 'too', 'which', 'all', 'me', &quot;mustn't&quot;, 'the', 'out', 'your', 'on', 'don', 'her', 'does', &quot;aren't&quot;, 'himself', 'from', 'further', 'here', 'mightn', &quot;it's&quot;, &quot;shouldn't&quot;, 'you', 'herself', 'ma', &quot;you're&quot;, 'have', 'because', 'myself', &quot;hasn't&quot;, 'there', 'him', 'against', 'them', 'can', 'as', 'those', 'isn', 'won', 'are', 'who', 'by', 've', 're', 'or', 'themselves', 'm', 'had', 'once', &quot;she's&quot;, 'couldn', 'she', 'am', 'during', 'off', 'that', 'our', 'under', 'needn', 'i', 'then', &quot;wouldn't&quot;, 'do', 'very', 'each', 'just', 'they', 'wasn', 'through', &quot;that'll&quot;, 'ain', 'whom', &quot;weren't&quot;, 'yourselves', 'didn', 'between', &quot;shan't&quot;, &quot;hadn't&quot;, 'before', 'but', &quot;doesn't&quot;, &quot;won't&quot;, 'shouldn', 's', 'been', 'if', 'hers', 'most', 'when', 'should', 'mustn', &quot;you'd&quot;, 'not', 'this', 'theirs', 'own', 'until', 'will', 'what', 'more', 'hasn', 'doesn', 'a', 'now', 'an', 'where', 'for', 'their', 'yours', 't', 'same', &quot;couldn't&quot;, 'above', &quot;isn't&quot;, 'll', 'nor', 'were', &quot;didn't&quot;, 'after', 'and', &quot;don't&quot;, 'other', &quot;mightn't&quot;, 'about', &quot;should've&quot;, 'of', 'while', 'aren', 'doing', 'few', 'in', 'both', 'with', 'over', 'haven', 'to', &quot;haven't&quot;, 'such', 'up', 'is', 'why', 'no', 'only', 'at', 'having', 'shan', 'be', 'ours', &quot;you've&quot;, 'yourself', 'we', 'd'}\n\n2.3. 정규화\n\n이전에 얘기했다시피 goes와 같은 3인칭 동사이지만 go와 같은 취급을 위해 변환을 하는 작업\n\n어간 추출\n\n어형이 변형된 단어로부터 접사등을 제거하고 그 단어의 어간을 분리해 내는 작업\n\n어형 : 단어의 형태를 의미\n\n어간 : 어형변화에서 변화하지 않는 부분 or 용언의 바뀌지 않는 부분\n\n용언 : 문장 안에서 서술하는 구실을 하는 동사와 형용사\n\n예시\n\n영어와 우리말은 원리와 구조가 달라서 어간 추출이 다르다.\n\n영어의 경우 명사가 복수형으로 기술된 것을 단수형으로 바꾸는 작업도 어간 추출에 포함\n\n포터 스테머(Porter Stemmer)\n\n마틴 포터가 작성한 스테밍 알고리즘\n단어가 변형되는 규칙을 이용해 원형을 찾는거라서 사전에 있는 단어로 변환 되는것은 아니다.\n\nfrom nltk.stem import PorterStemmer\nstemmer = PorterStemmer()\nprint(stemmer.stem('cooking'), stemmer.stem('cookery'),stemmer.stem('cookbooks'))\n# cook cookeri cookbook\n\n랭카스트 스테머(Lancaster Stemmer)\nfrom nltk.stem import LancasterStemmer\nstemmer = LancasterStemmer()\nprint(stemmer.stem('cooking'), stemmer.stem('cookery'),stemmer.stem('cookbooks'))\n# cook cookery cookbook\n\n두 알고리즘 차이는 cookery에서 볼수 있다.\n\n표제어 추출\n\n주어진 단어를 기본형으로 변환\n의미적 관점에서 단어의 기본형을 찾는 것\n\nfrom nltk.stem import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\nprint(lemmatizer.lemmatize('cooking'))\nprint(lemmatizer.lemmatize('cooking', pos='v')) # 품사를 지정\nprint(lemmatizer.lemmatize('cookery'))\nprint(lemmatizer.lemmatize('cookbooks'))\n# cooking\n# cook\n# cookery\n# cookbook\n\ncooking에 대한 기본형이 동일한데 사전에 요리라는 뜻으로 cooking이라는 명사가 존재하기 때문이고 pos에 동사라고 지정하면 cook이라고 반환한다.\n\n2.4. 품사 태깅\n\n토큰화와 정규화 과정을 거쳐서 나온 각 결과를 보통은 형태소라고 한다.\n\n형태소는 의미를 가진 가장 작은 말의 단위\n\n예시 ) 책가방 ⇒ 책 + 가방 ≠&gt;책 + 가 + 방\n\n품사의 이해\n\n주어진 텍스트를 분리할때 낱말까지 하는것이 좋은지 아니면 형태소까지 하는 것이 좋은지는 분석하고자 하는 내용과 성능에 달려 있다.\n아래는 우리말의 주요 품사와 설명이다.\n\n품사\n설명\n\n명사\n이름을 나타내는 낱말\n\n대명사\n이름을 대신해 가리키는 낱말\n\n수사\n수량이나 순서를 가리키는 낱말\n\n조사\n도와주는 낱말\n\n동사\n움직임을 나타내는 낱말\n\n형용사\n상태나 성질을 나타내는 낱말\n\n관형사\n체언을 꾸며 주는 낱말\n\n부사\n주로 용언을 꾸며주는 낱말\n\n감탄사\n놀람, 느낌, 부름, 대답을 나타내는 낱말\n\n용언 : 동사와 형용사를 함께 부르는 말\n체언 : 명사, 대명사, 수사를 묶어서 부르는 말\n조사 : 관계언\n감탄사 : 독립언\n공용 품사 태그\n\n품사 태그는 언어나 학자에 따라 다르게 정의함\n다양한 언어에서 공통되는 품사 태그를 나타낸 것\n\n태그\n뜻\n예\n\nADJ\nadjective\nnew, good, high, special, big, local\n\nADP\nadposition\non, of, at, with, by, into, under\n\nADV\nadverb\nreally, already, still, early, now\n\nCONJ\nconjunction\nand, or, but, if, while, although\n\nDET\ndeterminer, article\nthe, a, some, most, every, no, which\n\nNOUN\nnoun\nyear, home, costs, time, Africa\n\nNUM\nnumberal\ntwenty-four, fourth, 1991, 14:24\n\nPRT\nparticle\nat, on, out, over, per, that, up, with\n\nPRON\npronoun\nhe, their, her, its, my, I, us\n\nVERB\nverb\nis, say, told, given, playing, would\n\n.\npunctuation marks\n.,;!\n\nX\nother\nersatz, esprit, dunno, gr8, univeristy\n\n펜 트리뱅크 태그 집합\n\n공용 품사 태그 집합에 비해 훨씬 세분화된 품사 분류\nhttps://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n\nNLTK를 활용한 품사 태깅\n\n영어는 NLTK가 잘 해준다.\nNLTK는 펜 트리뱅크 태그 집합을 사용한다. 의미를 모르면 코드 마지막 처럼 쓸수 있다.\n\nimport nltk\nfrom nltk.tokenize import word_tokenize\nnltk.download('tagsets')\n\ntokens = word_tokenize(&quot;Hello everyone. It's good to see you. Let's start our text min-ing class!&quot;)\n\nprint(nltk.pos_tag(tokens))\n# [('Hello', 'NNP'), ('everyone', 'NN'), ('.', '.'), ('It', 'PRP'), (&quot;'s&quot;, 'VBZ'), ('good', 'JJ'), ('to', 'TO'), ('see', 'VB'), ('you', 'PRP'), ('.', '.'), ('Let', 'VB'), (&quot;'s&quot;, 'POS'), ('start', 'VB'), ('our', 'PRP$'), ('text', 'JJ'), ('min-ing', 'JJ'), ('class', 'NN'), ('!', '.')]\n\nnltk.help.upenn_tagset('NNP')\n&quot;&quot;&quot;\nNNP: noun, proper, singular\nMotown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\nOceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\nShannon A.K.C. Meltex Liverpool ...\n&quot;&quot;&quot;\n\n원하는 품사만 추출\n\nmy_tag_set = ['NN', 'VB', 'JJ']\nmy_words = [word for word, tag in nltk.pos_tag(tokens) if tag in my_tag_set]\nprint(my_words)\n# ['everyone', 'good', 'see', 'Let', 'start', 'text', 'mining', 'class']\n\n단어에 품사 정보를 추가해 구분\n\n동음이의어를 처리하거나 품사를 이용해 단어를 더 정확하게 구분하는 방법으로 문장의 의미를 정확히 파악하려고 할때 많이 쓰인다.\n\nwords_with_tag = ['/'.join(item) for item in nltk.pos_tag(tokens)]\nprint(words_with_tag)\n# ['Hello/NNP', 'everyone/NN', './.', 'It/PRP', &quot;'s/VBZ&quot;, 'good/JJ', 'to/TO', 'see/VB', 'you/PRP', './.', 'Let/VB', &quot;'s/POS&quot;, 'start/VB', 'our/PRP$', 'text/JJ', 'min-ing/JJ', 'class/NN', '!/.']\n\n한글 형태소 분석과 품사 태깅\n\nNLTK로는 제대로 안되므로 KoNLPy를 사용한다.\nKoNLPy는 5종의 형태소 분석기를 제공하며 홈페이지에서 성능 비교를 확인할 수 있다.\nmorphs(phrase) : 주어진 텍스트를 형태소 단위로 분리\nnouns(phrase) : 주어진 텍스트를 형태소 단위로 분리해서 명사만 반환\npos(phrase) : 주어진 텍스트를 형태소 단위로 분리하고 각 형태소에 부착해 반환\n\nfrom konlpy.tag import Okt\n\nt = Okt()\n\nsentence = '''절망의 반대가 희망은 아니다. 어두운 밤하늘에 별이 빛나듯\n희망은 절망 속에 싹트는 거지\n만약에 우리가 희망함이 적다면\n그 누가 세상을 비출어줄까. 정희성, 희망 공부'''\n\nprint('형태소:', t.morphs(sentence))\nprint()\nprint('명사:', t.nouns(sentence))\nprint()\nprint('품사 태깅 결과:', t.pos(sentence))\n\n&quot;&quot;&quot;\n형태소: ['절망', '의', '반대', '가', '희망', '은', '아니다', '.', '어', '두운', '밤하늘', '에', '별', '이', '빛나듯', '\\\\n', '희망', '은', '절망', '속', '에', '싹트는', '거지', '\\\\n', '만약', '에', '우리', '가', '희망', '함', '이', '적다면', '\\\\n', '그', '누가', '세상', '을', '비출어줄까', '.', '정희성', ',', '희망', '공부']\n\n명사: ['절망', '반대', '희망', '어', '두운', '밤하늘', '별', '희망', '절망', '속', '거지', '만약', '우리', '희망', '함', '그', '누가', '세상', '정희성', '희망', '공부']\n\n품사 태깅 결과: [('절망', 'Noun'), ('의', 'Josa'), ('반대', 'Noun'), ('가', 'Josa'), ('희망', 'Noun'), ('은', 'Josa'), ('아니다', 'Adjective'), ('.', 'Punctuation'), ('어', 'Noun'), ('두운', 'Noun'), ('밤하늘', 'Noun'), ('에', 'Josa'), ('별', 'Noun'), ('이', 'Josa'), ('빛나듯', 'Verb'), ('\\\\n', 'Foreign'), ('희망', 'Noun'), ('은', 'Josa'), ('절망', 'Noun'), ('속', 'Noun'), ('에', 'Josa'), ('싹트는', 'Verb'), ('거지', 'Noun'), ('\\\\n', 'Foreign'), ('만약', 'Noun'), ('에', 'Josa'), ('우리', 'Noun'), ('가', 'Josa'), ('희망', 'Noun'), ('함', 'Noun'), ('이', 'Josa'), ('적다면', 'Verb'), ('\\\\n', 'Foreign'), ('그', 'Noun'), ('누가', 'Noun'), ('세상', 'Noun'), ('을', 'Josa'), ('비출어줄까', 'Verb'), ('.', 'Punctuation'), ('정희성', 'Noun'), (',', 'Punctuation'), ('희망', 'Noun'), ('공부', 'Noun')]\n\n&quot;&quot;&quot;",
		"tags": [ "note"]
},

{
		"title": "asynchronous (비동기)",
		"date":"Thu Apr 04 2024 10:49:08 GMT+0000 (Coordinated Universal Time)",
		"url":"/Blogging/study/cs/asynchronous/",
		"content": "요청과 그 결과가 분리되어 처리되는 것\n요청을 보낸 후 다른 작업을 수행하다가 결과가 돌아오면 처리하는 것\n대규모 데이터 처리나 네트워크 통신 들에서 유용하게 사용",
		"tags": [ "note"]
},

{
		"title": "process (프로세스)",
		"date":"Thu Apr 04 2024 10:49:08 GMT+0000 (Coordinated Universal Time)",
		"url":"/Blogging/study/cs/process/",
		"content": "실행 중인 프로그램의 인스턴스\n자체 메모리 공간(코드, 데이터, 힙, 스택)을 가지며, 운영체제의 관리 하에 독립적으로 실행",
		"tags": [ "note"]
},

{
		"title": "synchronous (동기)",
		"date":"Thu Apr 04 2024 10:49:08 GMT+0000 (Coordinated Universal Time)",
		"url":"/Blogging/study/cs/synchronous/",
		"content": "요청과 그 결과가 한번에 처리되는 것\n요청을 보낸 후 결과가 돌아올 때까지 대기하는 것",
		"tags": [ "note"]
},

{
		"title": "thread (스레드)",
		"date":"Thu Apr 04 2024 10:49:08 GMT+0000 (Coordinated Universal Time)",
		"url":"/Blogging/study/cs/thread/",
		"content": "",
		"tags": [ "note"]
},

{
		"title": "Javascript에 대해서",
		"date":"Thu Apr 04 2024 10:49:08 GMT+0000 (Coordinated Universal Time)",
		"url":"/Blogging/study/language/Javascript에 대해서/",
		"content": "",
		"tags": [ "note"]
},

{
		"title": "Cache",
		"date":"Thu Apr 04 2024 10:49:08 GMT+0000 (Coordinated Universal Time)",
		"url":"/Blogging/study/web/Cache/",
		"content": "",
		"tags": [ "note"]
},

{
		"title": "DNS",
		"date":"Thu Apr 04 2024 10:49:08 GMT+0000 (Coordinated Universal Time)",
		"url":"/Blogging/study/web/DNS/",
		"content": "",
		"tags": [ "note"]
},

{
		"title": "Domain",
		"date":"Thu Apr 04 2024 10:49:08 GMT+0000 (Coordinated Universal Time)",
		"url":"/Blogging/study/web/Domain/",
		"content": "",
		"tags": [ "note"]
},

{
		"title": "HSTS",
		"date":"Thu Apr 04 2024 10:49:08 GMT+0000 (Coordinated Universal Time)",
		"url":"/Blogging/study/web/HSTS/",
		"content": "HSTS는 HTTP Strict Transport Security로 Web Site에 접속할 때, 강제적으로 HTTPS Protocol로만 접속하게 하는 기능입니다.",
		"tags": [ "note"]
},

{
		"title": "HTTP",
		"date":"Thu Apr 04 2024 10:49:08 GMT+0000 (Coordinated Universal Time)",
		"url":"/Blogging/study/web/HTTP/",
		"content": "",
		"tags": [ "note"]
},

{
		"title": "HTTPS",
		"date":"Thu Apr 04 2024 10:49:08 GMT+0000 (Coordinated Universal Time)",
		"url":"/Blogging/study/web/HTTPS/",
		"content": "",
		"tags": [ "note"]
},

{
		"title": "IP",
		"date":"Thu Apr 04 2024 10:49:08 GMT+0000 (Coordinated Universal Time)",
		"url":"/Blogging/study/web/IP/",
		"content": "",
		"tags": [ "note"]
},

{
		"title": "ISP",
		"date":"Thu Apr 04 2024 10:49:08 GMT+0000 (Coordinated Universal Time)",
		"url":"/Blogging/study/web/ISP/",
		"content": "",
		"tags": [ "note"]
},

{
		"title": "Router",
		"date":"Thu Apr 04 2024 10:49:08 GMT+0000 (Coordinated Universal Time)",
		"url":"/Blogging/study/web/Router/",
		"content": "",
		"tags": [ "note"]
},

{
		"title": "TCP",
		"date":"Thu Apr 04 2024 10:49:08 GMT+0000 (Coordinated Universal Time)",
		"url":"/Blogging/study/web/TCP/",
		"content": "",
		"tags": [ "note"]
},

{
		"title": "URL",
		"date":"Thu Apr 04 2024 10:49:08 GMT+0000 (Coordinated Universal Time)",
		"url":"/Blogging/study/web/URL/",
		"content": "",
		"tags": [ "note"]
},

{
		"title": "브라우저에 URL를 넣었을 때 어찌 되는가?",
		"date":"Thu Apr 04 2024 10:49:08 GMT+0000 (Coordinated Universal Time)",
		"url":"/Blogging/study/web/What is done when you enter a URL in your browser/",
		"content": "다음과 같은 순서로 진행 됩니다.\n\n브라우저에 <a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/Blogging/study/web/URL/\">URL</a> 주소를 넣습니다.\n브라우저에서 <a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/Blogging/study/web/URL/\">URL</a>를 해석합니다.\n\n<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/Blogging/study/web/URL/\">URL</a> 문법이 맞지 않다면 기본 검색 엔진으로 검색\n<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/Blogging/study/web/URL/\">URL</a> 문법이 맞다면, <a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/Blogging/study/web/URL/\">URL</a>의 호스트 부분을 인코딩합니다.\n\n<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/Blogging/study/web/HSTS/\">HSTS</a> 확인하고 있으면 <a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/Blogging/study/web/HTTPS/\">HTTPS</a>로 요청합니다.\n없으면 <a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/Blogging/study/web/HTTP/\">HTTP</a>로 요청합니다.\n\n브라우저는 캐싱 된 <a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/Blogging/study/web/DNS/\">DNS</a> 기록들에서 해당 <a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/Blogging/study/web/URL/\">URL</a>에 대응되는 <a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/Blogging/study/web/IP/\">IP</a> 주소가 있는지 확인합니다.\n\n<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/Blogging/study/web/DNS/\">DNS</a>의 목적은 편의성 제공이며, 캐싱은 네트워크 트래픽을 조절하고 데이터 전송 시간을 줄입니다.\nDNS query로 먼저 브라우저 <a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/Blogging/study/web/Cache/\">Cache</a>를 확인합니다.\n없다면 OS <a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/Blogging/study/web/Cache/\">Cache</a>를 확인합니다.\n없다면 <a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/Blogging/study/web/Router/\">Router</a> <a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/Blogging/study/web/Cache/\">Cache</a>를 확인합니다.\n없다면 <a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/Blogging/study/web/ISP/\">ISP</a> <a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/Blogging/study/web/Cache/\">Cache</a>를 확인합니다.\n\n요청한 <a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/Blogging/study/web/URL/\">URL</a>이 <a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/Blogging/study/web/Cache/\">Cache</a>에 없다면 <a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/Blogging/study/web/ISP/\">ISP</a>의 <a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/Blogging/study/web/DNS/\">DNS</a>서버가 해당 <a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/Blogging/study/web/Domain/\">Domain</a> 호스팅하고 있는 서버의 <a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/Blogging/study/web/IP/\">IP</a> 주소를 찾기 위해 DNS query를 ??에 보냅니다.\n브라우저가 서버와 <a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/Blogging/study/web/TCP/\">TCP</a> 커넥션을 합니다.\n브라우저가 웹 서버에 <a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/Blogging/study/web/HTTP/\">HTTP</a> 요청을 합니다.\n서버가 요청을 처리하고 <a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/Blogging/study/web/response/\">response</a>를 생성합니다.\n서버가 HTTP response를 보냅니다.\n브라우저가 받은 response의 body부분으로 content를 보여줍니다.\n\nDOM tree 생성을 합니다.\nCSSOM tree를 생성합니다.\n렌더링 tree 생성합니다.\nJavascript 파싱과 실행을 합니다.\n레이아웃을 그립니다.\n페이팅을 합니다.\n\n<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/Blogging/study/web/reflow/\">reflow</a>\n<a class=\"internal-link\" target=\"\" data-note-icon=\"\" href=\"/Blogging/study/web/repaint/\">repaint</a>\n\n#web #browser #interview #frontend",
		"tags": ["web", "browser", "interview", "frontend", "note"]
},

{
		"title": "reflow",
		"date":"Thu Apr 04 2024 10:49:08 GMT+0000 (Coordinated Universal Time)",
		"url":"/Blogging/study/web/reflow/",
		"content": "",
		"tags": [ "note"]
},

{
		"title": "repaint",
		"date":"Thu Apr 04 2024 10:49:08 GMT+0000 (Coordinated Universal Time)",
		"url":"/Blogging/study/web/repaint/",
		"content": "",
		"tags": [ "note"]
},

{
		"title": "request",
		"date":"Thu Apr 04 2024 10:49:08 GMT+0000 (Coordinated Universal Time)",
		"url":"/Blogging/study/web/request/",
		"content": "",
		"tags": [ "note"]
},

{
		"title": "response",
		"date":"Thu Apr 04 2024 10:49:08 GMT+0000 (Coordinated Universal Time)",
		"url":"/Blogging/study/web/response/",
		"content": "",
		"tags": [ "note"]
},

{
		"title": "Welcome",
		"date":"Thu Apr 04 2024 10:49:08 GMT+0000 (Coordinated Universal Time)",
		"url":"/",
		"content": "이곳은 공부를 한 것에 대해서 정리하거나 생각을 정리하는 곳입니다.\n아직은 빈 페이지가 많지만 열심히 채울 예정입니다.",
		"tags": [ "note","gardenEntry"]
}
]